# sample spark-cluster on the kubernetes

## environment settings

install a package

```shell
pip install kaggle
```

you set the mount path. (use absolute path!!!)

```spark-cluster.yaml
      volumes:
        - name: mount-directory
          hostPath:
            path: <path> ## mount-path
...
      volumes:
        - name: mount-data
          hostPath:
            path: <path> ## mount-path
```

## download data

set environment variables.

```shell
export KAGGLE_USERNAME=xxx
export KAGGLE_KEY=xxx
```

The parameters are generated by your account page. https://www.kaggle.com/<username>/account

```shell
bash download.sh
```

## deploy and run clusters

```shell
kubectl apply -f ./spark-cluster.yml
```

## show logs

```shell
kubectl get pods
```

```shell
kubectl logs <pods-name>
```

if you open jupyter notebook on your browser, you get the url from logs of jupyter-pyspark pod.

## delete resources

```shell
kubectl delete -f ./spark-cluster.yml
```
